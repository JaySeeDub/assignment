{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da098973",
   "metadata": {},
   "source": [
    "# Know thy Higgs boson\n",
    "\n",
    "**Background:** High-energy collisions at the Large Hadron Collider (LHC) produce particles that interact with particle detectors. One important task is to classify different types of collisions based on their physics content, allowing physicists to find patterns in the data and to potentially unravel new discoveries.\n",
    "\n",
    "**Problem statement:** The discovery of the Higgs boson by CMS and ATLAS Collaborations was announced at CERN in 2012. In this work, we focus on the potential of Machine Learning and Deep Learning in detecting potential Higgs signal from one of the background processes that mimics it.\n",
    "\n",
    "**Dataset:** The dataset is made available by the Center for Machine Learning and Intelligent Systems at University of California, Irvine. The dataset can be found on the [UCI Machine learning Repository](https://archive.ics.uci.edu/ml/datasets/HIGGS)\n",
    "\n",
    "**Description:** The dataset consists of a total of 11 million labeled samples of Higgs vs background events produced by Monte Carlo simulations. Each sample consists of 28 features. The first 21 features are kinematic properties measured at the level of the detectors. The last seven are functions of the first 21."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafb0ccb",
   "metadata": {},
   "source": [
    "**Steps to load the training dataset**\n",
    "1. **Download the dataset from the UCI website**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83b2eaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-15 11:01:20--  https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: 'HIGGS.csv.gz'\n",
      "\n",
      "HIGGS.csv.gz            [                <=> ]   2.62G  22.8MB/s    in 2m 20s  \n",
      "\n",
      "2025-06-15 11:03:41 (19.2 MB/s) - 'HIGGS.csv.gz' saved [2816407858]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6ec447",
   "metadata": {},
   "source": [
    "The above will create a file named `HIGGS.csv.gz`. Remember that in particle physics we deal with large amounts of data. In this case as well you have to be patient till the full file of `2.62 GB` data is downloaded. For a full analysis you might need to use the full dataset but for this exercise you can try with `10000` entries only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6421d693",
   "metadata": {},
   "source": [
    "2. **Unzip the folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71d47215",
   "metadata": {},
   "outputs": [],
   "source": [
    " !gzip -d -f HIGGS.csv.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50de8f41",
   "metadata": {},
   "source": [
    "The above will create a `HIGGS.csv` file in your local directory. At this point you will see that the `HIGGS.csv.gz` file is unzipped as `HIGGS.csv` and got deleted after unzipping is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22c5299",
   "metadata": {},
   "source": [
    "3. **Load libraries**\n",
    "\n",
    "If something is missing install them in the python environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94e69605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/cms.cern.ch/el9_amd64_gcc12/external/py3-numpy/1.24.3-a3a65cbc18c6a98e06451d4329e3ea47/lib/python3.9/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/cvmfs/cms.cern.ch/el9_amd64_gcc12/external/py3-numpy/1.24.3-a3a65cbc18c6a98e06451d4329e3ea47/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/cvmfs/cms.cern.ch/el9_amd64_gcc12/external/py3-numpy/1.24.3-a3a65cbc18c6a98e06451d4329e3ea47/lib/python3.9/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/cvmfs/cms.cern.ch/el9_amd64_gcc12/external/py3-numpy/1.24.3-a3a65cbc18c6a98e06451d4329e3ea47/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    ConfusionMatrixDisplay \n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)  # for reproducibility\n",
    "import h5py\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eadfe6",
   "metadata": {},
   "source": [
    "4. **Load the file using pandas library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('./HIGGS.csv', header=None)\n",
    "## print the first 5 rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e62beb1",
   "metadata": {},
   "source": [
    "The first column is the class label (1 for signal, 0 for background), followed by the 28 features (21 low-level features then 7 high-level features): lepton pT, lepton eta, lepton phi, missing energy magnitude, missing energy phi, jet 1 pt, jet 1 eta, jet 1 phi, jet 1 b-tag, jet 2 pt, jet 2 eta, jet 2 phi, jet 2 b-tag, jet 3 pt, jet 3 eta, jet 3 phi, jet 3 b-tag, jet 4 pt, jet 4 eta, jet 4 phi, jet 4 b-tag, m_jj, m_jjj, m_lv, m_jlv, m_bb, m_wbb, m_wwbb. **Try to find the meaning of each of the variables from the datasource**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319d676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## change the column numbers to column names\n",
    "data.columns = ['target',\n",
    "              'lep_pT',\n",
    "              'lep_eta',\n",
    "              'lep_phi',\n",
    "              'missE',\n",
    "              'missphi',\n",
    "              'jet1_pt',\n",
    "              'jet1_eta',\n",
    "              'jet_1_phi',\n",
    "              'jet_1_b_tag',\n",
    "              'jet_2_pt',\n",
    "              'jet_2_eta',\n",
    "              'jet_2_phi',\n",
    "              'jet_2_b_tag',\n",
    "              'jet_3_pt',\n",
    "              'jet_3_eta',\n",
    "              'jet_3_phi',\n",
    "              'jet_3_b_tag',\n",
    "              'jet_4_pt',\n",
    "              'jet_4_eta',\n",
    "              'jet_4_phi',\n",
    "              'jet_4_b_tag',\n",
    "              'm_jj',\n",
    "              'm_jjj',\n",
    "              'm_lv',\n",
    "              'm_jlv',\n",
    "              'm_bb',\n",
    "              'm_wbb',\n",
    "              'm_wwbb']\n",
    "\n",
    "## print the first 5 rows (including the column names)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89d7cb2",
   "metadata": {},
   "source": [
    "Assign first column $0$ to class labels (labeled $1$ for signal, $0$ for background)  and all others to feature matrix $X$.\n",
    "\n",
    "In this example, for the sake of fast checking, we use `10000` samples. To train on the entire dataset, proceed with uncommenting the lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32b453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.iloc[:10000,1:]#data.iloc[:,1:]\n",
    "y=data.iloc[:10000,0]#data.iloc[:,0]\n",
    "\n",
    "X_train1, X_val, y_train1, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train1, y_train1, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1cf5a9",
   "metadata": {},
   "source": [
    "5. **Visualize your data - One histogram per feature column**\n",
    "\n",
    "Detailed information on what each feature column is can be found in *Attribute Information* section on the [UCI Machine learning Repositery](https://archive.ics.uci.edu/ml/datasets/HIGGS). For further information, refer to the [paper](https://www.nature.com/articles/ncomms5308) by Baldi et. al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3ac7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(X_train.columns)//3, 3, figsize=(12, 48))\n",
    "\n",
    "i = 0\n",
    "for triaxis in axes:\n",
    "    for axis in triaxis:\n",
    "        X_train.hist(column = X_train.columns[i], bins = 100, ax=axis)\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13f8d62",
   "metadata": {},
   "source": [
    "You can also visualise it through `pairplot`, but only select a few features at a time otherwise it would take a long time to produce the pairplot. In the following I have presented an example of pairplot. Use other features to see the pairplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525c93ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the eta and phi columns for leptons and jets, plus the target for classification\n",
    "selected_columns = ['target',\n",
    "              'lep_pT',\n",
    "              #'lep_eta',\n",
    "              #'lep_phi',\n",
    "              #'missE',\n",
    "              #'missphi',\n",
    "              'jet1_pt',\n",
    "              #'jet1_eta',\n",
    "              #'jet_1_phi',\n",
    "              #'jet_1_b_tag',\n",
    "              'jet_2_pt',\n",
    "              #'jet_2_eta',\n",
    "              #'jet_2_phi',\n",
    "              #'jet_2_b_tag',\n",
    "              'jet_3_pt',\n",
    "              #'jet_3_eta',\n",
    "              #'jet_3_phi',\n",
    "              #'jet_3_b_tag',\n",
    "              'jet_4_pt',\n",
    "              #'jet_4_eta',\n",
    "              #'jet_4_phi',\n",
    "              #'jet_4_b_tag',\n",
    "              #'m_jj',\n",
    "              #'m_jjj',\n",
    "              #'m_lv',\n",
    "              #'m_jlv',\n",
    "              #'m_bb',\n",
    "              #'m_wbb',\n",
    "              #'m_wwbb'\n",
    "              ]\n",
    "\n",
    "# Select only those subset of the data\n",
    "pairplot_data = data[:500][selected_columns] # only taking the first 5000 data points\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=250)\n",
    "sns.pairplot(\n",
    "    pairplot_data,\n",
    "    hue=\"target\",\n",
    "    palette=\"Set1\",\n",
    "    diag_kind=\"hist\",\n",
    "    # Make dots smaller\n",
    "    plot_kws={\"s\": 10},\n",
    ")\n",
    "plt.suptitle(\"Pairplot of features\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a1351e",
   "metadata": {},
   "source": [
    "## **Setup the Boosted Decision Tree model** \n",
    "\n",
    "Boosted Decision Trees are explaned here [here](https://xgboost.readthedocs.io/en/stable/tutorials/model.html). This are what can be classified as `ensemble learning methods`, The main idea is to combine many weak learners (typically shallow decision trees) to create a strong classifier. In the following we will use **XGBoost = \"Extreme Gradient Boosting\"** package. \n",
    "\n",
    "For each round (up to `n_estimators`):\n",
    "1. **Compute Gradient (error/loss derivative):** Measures how wrong the current prediction is using the weak learners with `max_depth`. This sets the maximum depth of each individual decision tree (not the number of trees).\n",
    "\n",
    "- A tree of depth $3$ has at most $2^3 = 8$ leaf nodes.\n",
    "- It controls the complexity of each tree:\n",
    "    - Smaller depth → simpler trees, less overfitting.\n",
    "    - Larger depth → more complex trees, more capacity, but risk of overfitting.\n",
    "\n",
    "**Intuition:** If each tree is a `correction rule`, `max_depth` limits how detailed each rule can be.\n",
    "\n",
    "2. **Fit a New Tree:** A decision tree is trained to predict the gradient (residual) of the loss function.\n",
    "\n",
    "3. **Update Predictions:** Add the tree’s output to the model’s prediction (with some scaling factor: learning_rate).\n",
    "\n",
    "4. **Regularization:** Penalize complex trees to avoid overfitting (e.g., with max_depth, min_child_weight, gamma).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4278009",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'  # or 'mlogloss' for multi-class\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0537dbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\") # I am suppressing some warning, but you can let it run\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c89dc3",
   "metadata": {},
   "source": [
    "Now it is the time to predict the class using the classifier and see how it performs. \n",
    "\n",
    "**Objective: Improve the model accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c618399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "# Confusion Matrix, but this time using seaborn\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute and print scores\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "prec = precision_score(y_test, predictions, average='binary')  \n",
    "rec = recall_score(y_test, predictions, average='binary')\n",
    "f1 = f1_score(y_test, predictions, average='binary')\n",
    "\n",
    "print(f\"Accuracy  : {acc:.4f}\")\n",
    "print(f\"Precision : {prec:.4f}\")\n",
    "print(f\"Recall    : {rec:.4f}\")\n",
    "print(f\"F1 Score  : {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aceee2",
   "metadata": {},
   "source": [
    "## Setup the neural network model\n",
    "\n",
    "**Objective:** Use pytorch to build a network to predict the classes and compare that with bossted tree output.\n",
    "\n",
    "**Hints:**\n",
    "- you might want to use the Binary cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e409df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(29, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 29),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(29, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = SimpleNet()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.BGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd93cd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de702b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
